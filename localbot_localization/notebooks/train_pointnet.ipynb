{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://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@github.com/DanielCoelho112/localization_end_to_end.git\n",
    "#! pip install --user git+https://github.com/DanielPollithy/pypcd.git\n",
    "#! pip install colorama\n",
    "\n",
    "# add localization_end_to_end to all imports\n",
    "\n",
    "# change dataset directory to '/content/'\n",
    "\n",
    "# add from pypcd import pypcd in utilities\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from localization_end_to_end.localbot_localization.src.models.pointnet import PointNet, feature_transform_regularizer\n",
    "from localization_end_to_end.localbot_localization.src.dataset import LocalBotDataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports using Local Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from localbot_localization.src.models.pointnet import PointNet, feature_transform_regularizer\n",
    "from localbot_localization.src.dataset import LocalBotDataset\n",
    "from localbot_localization.src.loss_functions import BetaLoss, DynamicLoss\n",
    "from localbot_localization.src.utilities import *\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LocalBotDataset(seq=110, npoints=2000)\n",
    "test_dataset = LocalBotDataset(seq=111, npoints=2000)\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,  num_workers=2)\n",
    "# Num_workers tells the data loader instance how many sub-processes to use for data loading. If the num_worker is zero (default) the GPU has to wait for CPU to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_transform = False # lets use feature transform\n",
    "model = PointNet(feature_transform=feature_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.MSELoss() # TODO: search for a better loss function!! Should we predict the translaction and rotation separately?? RESEARCH!\n",
    "#criterion = BetaLoss()\n",
    "criterion = DynamicLoss()\n",
    "\n",
    "# Add all params for optimization\n",
    "param_list = [{'params': model.parameters()}]\n",
    "if isinstance(criterion, DynamicLoss):\n",
    "    # Add sx and sq from loss function to optimizer params\n",
    "    param_list.append({'params': criterion.parameters()})\n",
    "\n",
    "optimizer = optim.Adam(params = param_list, lr=0.001) # the most common optimizer in DL\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5) # variable learning rate. After 5 epochs, the lr decays 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda() # move all model parameters to the GPU\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch = len(train_dataset) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "train_losses = np.zeros(n_epochs)\n",
    "test_losses = np.zeros(n_epochs)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    t0 = datetime.now()\n",
    "    scheduler.step() # here we are telling the scheduler that: n_epochs += 1\n",
    "    train_loss = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        points, target = data\n",
    "        #points.shape --> 2,10000,3\n",
    "        #target.shape --> 2,6\n",
    "    \n",
    "        points = points.transpose(2, 1) # 3xN which is what our network is expecting\n",
    "        points, target = points.cuda(), target.cuda() # move data into GPU\n",
    "        \n",
    "        optimizer.zero_grad() # Clears the gradients of all optimized tensors (always needed in the beginning of the training loop)\n",
    "        \n",
    "        model = model.train() # Sets the module in training mode. For example, the dropout module can only be use in training mode.\n",
    "        \n",
    "        #print(points.shape)\n",
    "        pred, trans, trans_feat = model(points) # our model outputs the pose, and the transformations used\n",
    "        \n",
    "        pred = process_pose(pred)\n",
    "            \n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        if feature_transform:\n",
    "            loss += feature_transform_regularizer(trans_feat) * 0.001 ## Regularization! --> Prevent overfitting by adding something to the cost function. The simpler the model the lower the cost function\n",
    "        \n",
    "        \n",
    "        loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "        optimizer.step() # Performs a single optimization step (parameter update).\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    test_loss=[]\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        points, target = data\n",
    "        points = points.transpose(2, 1) # 3xN which is what our network is expecting\n",
    "        points, target = points.cuda(), target.cuda() # move data into GPU\n",
    "        model = model.eval() # Sets the module in evaluation mode.\n",
    "\n",
    "        pred, _, _ = model(points)\n",
    "        \n",
    "        pred = process_pose(pred)\n",
    "        \n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        test_loss.append(loss.item())\n",
    "    test_loss = np.mean(test_loss)\n",
    "    \n",
    "\n",
    "    # save losses\n",
    "    train_losses[epoch] = train_loss\n",
    "    test_losses[epoch] = test_loss\n",
    "    \n",
    "    dt = datetime.now() - t0\n",
    "    print(f'epoch {epoch+1}/{n_epochs}, train_loss: {train_loss:.4f}, test_loss: {test_loss:.4f}, duration: {dt}')\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pointnet.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = Variable(torch.rand(1,3,10000)).cuda()   # batch size = 32, 3 features, n_points = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sim_data)[0].cpu().detach().numpy()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
